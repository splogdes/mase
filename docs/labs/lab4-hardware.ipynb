{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://deepwok.github.io/\">\n",
    "    <img src=\"../imgs/deepwok.png\" alt=\"Logo\" width=\"160\" height=\"160\">\n",
    "  </a>\n",
    "\n",
    "  <h1 align=\"center\">Lab 4 for Advanced Deep Learning Systems (ADLS) - Hardware Stream</h1>\n",
    "\n",
    "  <p align=\"center\">\n",
    "    ELEC70109/EE9-AML3-10/EE9-AO25\n",
    "    <br />\n",
    "\t\tWritten by\n",
    "    <a href=\"https://aaron-zhao123.github.io/\">Aaron Zhao, Pedro Gimenes </a>\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General introduction\n",
    "\n",
    "In this lab, you will learn how to emit SystemVerilog code for a neural network that's been transformed and optimized by MASE. Then, you'll design some hardware for a new Pytorch layer, and simulate the hardware using your new module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hardware Emit pass\n",
    "\n",
    "The `emit_verilog` transform pass generates a top-level RTL file and testbench file according to the `MaseGraph`, which includes a hardware implementation of each layer in the network. This top-level file instantiates modules from the `components` library in MASE and/or modules generated using [HLS](https://en.wikipedia.org/wiki/High-level_synthesis), when internal components are not available. The hardware can then be simulated using [Verilator](https://www.veripool.org/verilator/), or deployed on an FPGA.\n",
    "\n",
    "First, add Machop to your system PATH (if you haven't already done so) and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to debug\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "        verilator --help\n",
      "        verilator --version\n",
      "        verilator --binary -j 0 [options] [source_files.v]... [opt_c_files.cpp/c/cc/a/o/so]\n",
      "        verilator --cc [options] [source_files.v]... [opt_c_files.cpp/c/cc/a/o/so]\n",
      "        verilator --sc [options] [source_files.v]... [opt_c_files.cpp/c/cc/a/o/so]\n",
      "        verilator --lint-only -Wall [source_files.v]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "\n",
    "from chop.ir.graph.mase_graph import MaseGraph\n",
    "\n",
    "from chop.passes.graph.analysis import (\n",
    "    init_metadata_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    add_hardware_metadata_analysis_pass,\n",
    "    report_node_type_analysis_pass,\n",
    ")\n",
    "\n",
    "from chop.passes.graph.transforms import (\n",
    "    emit_verilog_top_transform_pass,\n",
    "    emit_internal_rtl_transform_pass,\n",
    "    emit_bram_transform_pass,\n",
    "    emit_cocotb_transform_pass,\n",
    "    quantize_transform_pass,\n",
    ")\n",
    "\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "\n",
    "set_logging_verbosity(\"debug\")\n",
    "\n",
    "import toml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# TO DO: remove\n",
    "import os\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin:\" + os.environ[\"PATH\"]\n",
    "!verilator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the neural network. We're using a model which can be used to perform digit classification on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Toy FC model for digit recognition on MNIST\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(8, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        # x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll generate a MaseGraph and add metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running `emit_verilog`, we'll quantize the model to fixed precision. Refer back to [lab 3](https://deepwok.github.io/mase/modules/labs_2023/lab3.html) if you've forgotten how this works. Check that the data type for each node is correct after quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mgraph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})\n",
      "    return fc1\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInspecting graph [add_common_node_type_analysis_pass]\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Node name    Fx Node op    Mase type            Mase op      Value type\n",
      "-----------  ------------  -------------------  -----------  ------------\n",
      "x            placeholder   placeholder          placeholder  NA\n",
      "fc1          call_module   module_related_func  linear       mxint\n",
      "output       output        output               output       NA\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x {'mase_type': 'placeholder', 'mase_op': 'placeholder', 'args': {}, 'results': OrderedDict([('data_out_0', {'type': 'float', 'precision': [32], 'shape': [4, 8], 'torch_dtype': torch.float32})])}\n",
      "fc1 {'mase_type': 'module_related_func', 'mase_op': 'linear', 'args': OrderedDict([('data_in_0', {'shape': [4, 8], 'torch_dtype': torch.float32, 'type': 'mxint', 'precision': [12, 4]}), ('weight', {'type': 'mxint', 'precision': [12, 4], 'shape': [8, 8], 'from': None}), ('bias', {'type': 'mxint', 'precision': [12, 4], 'shape': [1, 8], 'from': None})]), 'results': OrderedDict([('data_out_0', {'type': 'mxint', 'precision': [12, 4], 'shape': [4, 8], 'torch_dtype': torch.float32})])}\n",
      "output {'mase_type': 'output', 'mase_op': 'output', 'args': {}, 'results': OrderedDict([('data_out_0', {'type': 'float', 'precision': [32], 'shape': [4, 8], 'torch_dtype': torch.float32})])}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mg = MaseGraph(model=mlp)\n",
    "\n",
    "# Provide a dummy input for the graph so it can use for tracing\n",
    "batch_size = 4\n",
    "x = torch.randn((batch_size, 8))\n",
    "dummy_in = {\"x\": x}\n",
    "mlp.forward(x)\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(\n",
    "    mg, {\"dummy_in\": dummy_in, \"add_value\": False}\n",
    ")\n",
    "\n",
    "quan_args = {\n",
    "        \"by\": \"type\",\n",
    "        \"default\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"mxint\",\n",
    "                # data\n",
    "                \"data_in_width\": 12,\n",
    "                \"data_in_exponent_width\": 4,\n",
    "                \"weight_block_size\": [1, 2],\n",
    "                # weight\n",
    "                \"weight_width\": 12,\n",
    "                \"weight_exponent_width\": 4,\n",
    "                \"bias_block_size\": [2, 2],\n",
    "                # bias\n",
    "                \"bias_width\": 12,\n",
    "                \"bias_exponent_width\": 4,\n",
    "                \"data_in_block_size\": [1, 2],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "mg, _ = quantize_transform_pass(mg, quan_args)\n",
    "\n",
    "_ = report_node_type_analysis_pass(mg)\n",
    "\n",
    "for node in mg.fx_graph.nodes:\n",
    "    # if not node.meta['mase']['hardware']['is_implicit']:\n",
    "    print(node, node.meta['mase']['common'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it's important to run the `add_hardware_metadata` analysis pass. This adds all the required metadata which is later used by the `emit_verilog` pass, including:\n",
    "\n",
    "1. The node's toolchain, which defines whether we use internal Verilog modules from the `components` library or the HLS flow.\n",
    "2. The Verilog parameters associated with each node.\n",
    "\n",
    "> **_TASK:_** Read [this page](https://deepwok.github.io/mase/modules/chop/analysis/add_metadata.html#add-hardware-metadata-analysis-pass) for more information on the hardware metadata pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mInspecting graph [add_common_node_shape_analysis_pass]\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mx:\n",
      "in:\n",
      "out:\n",
      "data_out_0 = [4, 8]\n",
      "\n",
      "fc1:\n",
      "in:\n",
      "data_in_0 = [4, 8]\n",
      "weight = {'type': 'mxint', 'precision': [12, 4], 'shape': [8, 8], 'from': None}\n",
      "bias = {'type': 'mxint', 'precision': [12, 4], 'shape': [1, 8], 'from': None}\n",
      "out:\n",
      "data_out_0 = [4, 8]\n",
      "\n",
      "output:\n",
      "in:\n",
      "out:\n",
      "data_out_0 = [4, 8]\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "fc1\n",
      "output\n",
      "fc1 {'mase_type': 'module_related_func', 'mase_op': 'linear', 'args': OrderedDict([('data_in_0', {'shape': [4, 8], 'torch_dtype': torch.float32, 'type': 'mxint', 'precision': [12, 4]}), ('weight', {'type': 'mxint', 'precision': [12, 4], 'shape': [8, 8], 'from': None}), ('bias', {'type': 'mxint', 'precision': [12, 4], 'shape': [1, 8], 'from': None})]), 'results': OrderedDict([('data_out_0', {'type': 'mxint', 'precision': [12, 4], 'shape': [4, 8], 'torch_dtype': torch.float32})])} {\n",
      "  \"is_implicit\": false,\n",
      "  \"device_id\": -1,\n",
      "  \"interface\": {\n",
      "    \"weight\": {\n",
      "      \"storage\": \"BRAM\",\n",
      "      \"transpose\": false\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"storage\": \"BRAM\",\n",
      "      \"transpose\": false\n",
      "    }\n",
      "  },\n",
      "  \"toolchain\": \"INTERNAL_RTL\",\n",
      "  \"module\": \"mxint_linear\",\n",
      "  \"dependence_files\": [\n",
      "    \"linear_layers/mxint_operators/rtl/mxint_linear.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/mxint_circular.sv\",\n",
      "    \"memory/rtl/input_buffer.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/mxint_dot_product.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/mxint_accumulator.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/mxint_cast.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/log2_max_abs.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/or_tree.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/or_tree_layer.sv\",\n",
      "    \"linear_layers/mxint_operators/rtl/mxint_register_slice.sv\",\n",
      "    \"common/rtl/unpacked_register_slice.sv\",\n",
      "    \"common/rtl/split2.sv\",\n",
      "    \"common/rtl/join2.sv\",\n",
      "    \"memory/rtl/unpacked_skid_buffer.sv\",\n",
      "    \"memory/rtl/skid_buffer.sv\",\n",
      "    \"memory/rtl/ultraram_fifo.sv\",\n",
      "    \"memory/rtl/ultraram.v\"\n",
      "  ],\n",
      "  \"max_parallelism\": [\n",
      "    4,\n",
      "    4,\n",
      "    4,\n",
      "    4\n",
      "  ],\n",
      "  \"verilog_param\": {\n",
      "    \"DATA_IN_0_PRECISION_0\": 12,\n",
      "    \"DATA_IN_0_PRECISION_1\": 4,\n",
      "    \"DATA_IN_0_TENSOR_SIZE_DIM_0\": 8,\n",
      "    \"DATA_IN_0_PARALLELISM_DIM_0\": 4,\n",
      "    \"DATA_IN_0_TENSOR_SIZE_DIM_1\": 4,\n",
      "    \"DATA_IN_0_PARALLELISM_DIM_1\": 4,\n",
      "    \"WEIGHT_PRECISION_0\": 12,\n",
      "    \"WEIGHT_PRECISION_1\": 4,\n",
      "    \"WEIGHT_TENSOR_SIZE_DIM_0\": 8,\n",
      "    \"WEIGHT_PARALLELISM_DIM_0\": 4,\n",
      "    \"WEIGHT_TENSOR_SIZE_DIM_1\": 8,\n",
      "    \"WEIGHT_PARALLELISM_DIM_1\": 4,\n",
      "    \"BIAS_PRECISION_0\": 12,\n",
      "    \"BIAS_PRECISION_1\": 4,\n",
      "    \"BIAS_TENSOR_SIZE_DIM_0\": 8,\n",
      "    \"BIAS_PARALLELISM_DIM_0\": 4,\n",
      "    \"BIAS_TENSOR_SIZE_DIM_1\": 1,\n",
      "    \"BIAS_PARALLELISM_DIM_1\": 1,\n",
      "    \"DATA_OUT_0_PRECISION_0\": 12,\n",
      "    \"DATA_OUT_0_PRECISION_1\": 4,\n",
      "    \"DATA_OUT_0_TENSOR_SIZE_DIM_0\": 8,\n",
      "    \"DATA_OUT_0_PARALLELISM_DIM_0\": 4,\n",
      "    \"DATA_OUT_0_TENSOR_SIZE_DIM_1\": 4,\n",
      "    \"DATA_OUT_0_PARALLELISM_DIM_1\": 4\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from chop.passes.graph.analysis.report import report_node_shape_analysis_pass\n",
    "mg, _ = add_hardware_metadata_analysis_pass(mg)\n",
    "\n",
    "report_node_shape_analysis_pass(mg, {})\n",
    "\n",
    "for node in mg.fx_graph.nodes:\n",
    "    if not node.meta['mase']['hardware']['is_implicit']:\n",
    "        print(node, node.meta['mase']['common'], json.dumps(node.meta['mase']['hardware'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the emit verilog pass to generate the SystemVerilog files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mEmitting Verilog...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEmitting internal components...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, _ = emit_verilog_top_transform_pass(mg)\n",
    "mg, _ = emit_internal_rtl_transform_pass(mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated files should now be found under `top/hardware`. \n",
    "\n",
    "> **_TASK:_** Read through `top/hardware/rtl/top.sv` and make sure you understand how our MLP model maps to this hardware design. \n",
    "\n",
    "You will notice the following instantiated modules:\n",
    "\n",
    "* `fixed_linear`: this is found under `components/linear/fixed_linear.sv` and implements each Linear layer in the model.\n",
    "* `fc<layer number>_weight/bias_source`: these are [BRAM](https://nandland.com/lesson-15-what-is-a-block-ram-bram/) memories which drive the weights and biases into the linear layers for computation.\n",
    "* `fixed_relu`: found under `components/activations/fixed_relu.sv`, implements the ReLU activation.\n",
    "\n",
    "As of now, we can't yet run a simulation on the model, as we haven't yet generated the memory components. To do this, run the `emit_bram` transform pass as follows, which will generate the memory initialization files and SystemVerilog modules to drive weights and biases into the linear layers. Finally, the `emit_verilog_tb` transform pass will generate the testbench files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mEmitting BRAM...\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mEmitting DAT file for node: fc1, parameter: weight\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mInit data weight successfully written into /home/splogdes/.mase/top/hardware/rtl/fc1_weight_rom_block.dat\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mInit data weight successfully written into /home/splogdes/.mase/top/hardware/rtl/fc1_weight_rom_exp.dat\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mEmitting DAT file for node: fc1, parameter: bias\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mInit data bias successfully written into /home/splogdes/.mase/top/hardware/rtl/fc1_bias_rom_block.dat\u001b[0m\n",
      "\u001b[36mDEBUG   \u001b[0m \u001b[34mInit data bias successfully written into /home/splogdes/.mase/top/hardware/rtl/fc1_bias_rom_exp.dat\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -11.,  -558.,  -939.,  -845.],\n",
      "        [  777.,   388.,  -667.,  1245.],\n",
      "        [-1192.,   -29., -1012.,   646.],\n",
      "        [-1066.,  1148., -1356.,   702.],\n",
      "        [ -129., -1383.,    76., -1046.],\n",
      "        [  383.,  -959.,  -742.,  -747.],\n",
      "        [ -438.,  -597.,   245.,   914.],\n",
      "        [ -285.,    54., -1352.,   849.],\n",
      "        [  573.,   526.,  -642.,   575.],\n",
      "        [  869.,  1203.,   -52.,   196.],\n",
      "        [ -982.,  -298.,   926.,   971.],\n",
      "        [ -631.,  1084.,  1440.,  -853.],\n",
      "        [ -233.,  -912.,   270.,   655.],\n",
      "        [  153.,  -367., -1123.,   582.],\n",
      "        [ 1311.,  -564., -1004.,  -858.],\n",
      "        [-1343.,  1251.,  -748.,   437.]])\n",
      "tensor([[5., 5., 5., 5.]])\n",
      "tensor([[ 1590.,  -366.,   111.,   671.],\n",
      "        [  898.,  1391., -1116.,  -531.]])\n",
      "tensor([[4.],\n",
      "        [5.]])\n"
     ]
    }
   ],
   "source": [
    "mg, _ = emit_bram_transform_pass(mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mEmitting testbench...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mg, _ = emit_cocotb_transform_pass(mg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_TASK:_** Now, you're ready to launch a simulation by calling the simulate action as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running command perl /usr/bin/verilator -cc --exe -Mdir /home/splogdes/Documents/UNI/ADL/mase/docs/labs/sim_build -DCOCOTB_SIM=1 --top-module top --vpi --public-flat-rw --prefix Vtop -o top -LDFLAGS '-Wl,-rpath,/home/splogdes/Documents/UNI/ADL/venv/lib/python3.11/site-packages/cocotb/libs -L/home/splogdes/Documents/UNI/ADL/venv/lib/python3.11/site-packages/cocotb/libs -lcocotbvpi_verilator' -Wno-fatal -Wno-lint -Wno-style --trace-fst --trace-structs --trace-depth 3 -I/home/splogdes/.mase/top/hardware/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/vivado/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/normalization_layers/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/cast/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/activation_layers/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/hls/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/systolic_arrays/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/memory/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/scalar_operators/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/linear_layers/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/interface/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/helper/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/transformer_layers/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/language_models/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/convolution_layers/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/vision_models/rtl -I/home/splogdes/Documents/UNI/ADL/mase/src/mase_components/common/rtl /home/splogdes/Documents/UNI/ADL/venv/lib/python3.11/site-packages/cocotb/share/lib/verilator/verilator.cpp /home/splogdes/.mase/top/hardware/rtl/fixed_adder_tree.sv /home/splogdes/.mase/top/hardware/rtl/or_tree.sv /home/splogdes/.mase/top/hardware/rtl/fixed_mult.sv /home/splogdes/.mase/top/hardware/rtl/split2.sv /home/splogdes/.mase/top/hardware/rtl/top.sv /home/splogdes/.mase/top/hardware/rtl/input_buffer.sv /home/splogdes/.mase/top/hardware/rtl/fc1_bias_source.sv /home/splogdes/.mase/top/hardware/rtl/register_slice.sv /home/splogdes/.mase/top/hardware/rtl/fixed_relu.sv /home/splogdes/.mase/top/hardware/rtl/unpacked_register_slice.sv /home/splogdes/.mase/top/hardware/rtl/mxint_accumulator.sv /home/splogdes/.mase/top/hardware/rtl/unpacked_skid_buffer.sv /home/splogdes/.mase/top/hardware/rtl/matrix_accumulator.sv /home/splogdes/.mase/top/hardware/rtl/fixed_vector_mult.sv /home/splogdes/.mase/top/hardware/rtl/fixed_linear.sv /home/splogdes/.mase/top/hardware/rtl/fc1_weight_source.sv /home/splogdes/.mase/top/hardware/rtl/mxint_dot_product.sv /home/splogdes/.mase/top/hardware/rtl/fixed_accumulator.sv /home/splogdes/.mase/top/hardware/rtl/mxint_circular.sv /home/splogdes/.mase/top/hardware/rtl/matmul.sv /home/splogdes/.mase/top/hardware/rtl/transpose.sv /home/splogdes/.mase/top/hardware/rtl/simple_matmul.sv /home/splogdes/.mase/top/hardware/rtl/matrix_flatten.sv /home/splogdes/.mase/top/hardware/rtl/skid_buffer.sv /home/splogdes/.mase/top/hardware/rtl/fixed_cast.sv /home/splogdes/.mase/top/hardware/rtl/mxint_register_slice.sv /home/splogdes/.mase/top/hardware/rtl/fixed_dot_product.sv /home/splogdes/.mase/top/hardware/rtl/join2.sv /home/splogdes/.mase/top/hardware/rtl/unpacked_repeat_circular_buffer.sv /home/splogdes/.mase/top/hardware/rtl/matrix_fifo.sv /home/splogdes/.mase/top/hardware/rtl/or_tree_layer.sv /home/splogdes/.mase/top/hardware/rtl/matrix_unflatten.sv /home/splogdes/.mase/top/hardware/rtl/matrix_stream_transpose.sv /home/splogdes/.mase/top/hardware/rtl/fixed_adder_tree_layer.sv /home/splogdes/.mase/top/hardware/rtl/log2_max_abs.sv /home/splogdes/.mase/top/hardware/rtl/mxint_linear.sv /home/splogdes/.mase/top/hardware/rtl/mxint_cast.sv /home/splogdes/.mase/top/hardware/rtl/ultraram_fifo.sv in directory /home/splogdes/Documents/UNI/ADL/mase/docs/labs/sim_build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%Error: /home/splogdes/.mase/top/hardware/rtl/log2_max_abs.sv:36:3: syntax error, unexpected ')', expecting IDENTIFIER-for-type\n",
      "   36 |   ) max_bas_i (\n",
      "      |   ^\n",
      "%Error: /home/splogdes/.mase/top/hardware/rtl/log2_max_abs.sv:48:3: syntax error, unexpected ')', expecting IDENTIFIER-for-type\n",
      "   48 |   ) log2_i (\n",
      "      |   ^\n",
      "%Error: /home/splogdes/.mase/top/hardware/rtl/mxint_cast.sv:49:3: syntax error, unexpected ')', expecting IDENTIFIER-for-type\n",
      "   49 |   ) max_bas_i (\n",
      "      |   ^\n",
      "%Error: Exiting due to 3 error(s)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Process 'perl' terminated with error 1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Process 'perl' terminated with error 1\n"
     ]
    }
   ],
   "source": [
    "from chop.actions import simulate\n",
    "\n",
    "simulate(skip_build=False, skip_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `simulate` action creates a `dump.vcd` file within the `sim_build` directory, which contains the waveform trace of the simulation. The waveforms can be opened with a viewer like GTKWave.\n",
    "\n",
    "> **TASK**: Follow the instructions [here](https://gtkwave.sourceforge.net/) to install GTKWave on your platform, then open the generated trace file to inspect the signals in the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Task\n",
    "\n",
    "Pytorch has a number of layers which are available to users to define neural network models. At the moment, `emit_verilog` supports generating Verilog for models including Linear layers and the ReLU activation.\n",
    "\n",
    "> **_MAIN TASK:_** choose another layer type from the [Pytorch list](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) and write a SystemVerilog file to implement that layer in hardware. Then, change the generated `top.sv` file to inject that layer within the design. For example, you may replace the ReLU activations with [Leaky ReLU](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU). Re-run the simulation and observe the effect on latency and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
